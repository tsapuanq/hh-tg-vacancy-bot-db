# #parser.py
# from playwright.async_api import async_playwright
# from src.config import BASE_URL, REGION_ID
# import logging

# async def get_vacancy_links(keyword: str, max_pages: int = 10) -> list[str]:
#     links = []

#     async with async_playwright() as p:
#         browser = await p.chromium.launch(headless=True)
#         page = await browser.new_page()

#         for page_number in range(max_pages):
#             url = f"{BASE_URL}?text={keyword}&area={REGION_ID}&page={page_number}&order_by=publication_time&search_period=1"
#             logging.info(f"–ü–∞—Ä—Å–∏–º: {url}")
#             await page.goto(url)
#             try:
#                 await page.wait_for_selector('a[data-qa="serp-item__title"]', timeout=5000)
#             except:
#                 logging.warning(f"–ù–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number}")
#                 break

#             items = await page.query_selector_all('a[data-qa="serp-item__title"]')
#             page_links = [await item.get_attribute("href") for item in items if await item.get_attribute("href")]
#             links.extend(page_links)

#         await browser.close()

#     return links

# parser.py
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from src.config import BASE_URL, REGION_ID, SCRAPER_TIMEOUT_SELECTOR
import logging

async def get_vacancy_links(keyword: str, max_pages: int = 10) -> list[str]:
    """
    –°–æ–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–∏—Å–∫–∞.
    """
    links = []
    logging.info(f"üîó –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É: '{keyword}' –¥–æ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü.")

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º try...finally –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏–∏ –∑–∞–∫—Ä—ã—Ç–∏—è –±—Ä–∞—É–∑–µ—Ä–∞
        try:
            page = await browser.new_page()

            for page_number in range(max_pages):
                url = f"{BASE_URL}?text={keyword}&area={REGION_ID}&page={page_number}&order_by=publication_time&search_period=1"
                logging.info(f"–ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_number + 1}/{max_pages}: {url}") # –õ–æ–≥–∏—Ä—É–µ–º –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã

                try:
                    await page.goto(url, timeout=20000) # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–∞–π–º–∞—É—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–ª–∏ –∏–∑ config
                    await page.wait_for_selector('a[data-qa="serp-item__title"]', timeout=SCRAPER_TIMEOUT_SELECTOR) # –¢–∞–π–º–∞—É—Ç –¥–ª—è —Å–µ–ª–µ–∫—Ç–æ—Ä–∞
                except PlaywrightTimeoutError:
                     logging.warning(f"‚è≥ –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number + 1} –¥–ª—è {keyword}. –í–æ–∑–º–æ–∂–Ω–æ, –Ω–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–π –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–Ω–∏–º–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.")
                     # –ï—Å–ª–∏ –Ω–µ—Ç —Å–µ–ª–µ–∫—Ç–æ—Ä–∞, –≤–µ—Ä–æ—è—Ç–Ω–æ, —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å –∏–ª–∏ –æ—à–∏–±–∫–∞. –ú–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å break.
                     break # –ó–∞–∫–∞–Ω—á–∏–≤–∞–µ–º, –µ—Å–ª–∏ –Ω–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–π –∏–ª–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω
                except Exception as e:
                    logging.warning(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number + 1} –¥–ª—è {keyword}: {e}")
                    continue # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é

                try:
                    items = await page.query_selector_all('a[data-qa="serp-item__title"]')
                    page_links = [await item.get_attribute("href") for item in items if await item.get_attribute("href")]
                    logging.info(f"‚ö° –ù–∞–π–¥–µ–Ω–æ {len(page_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number + 1}")
                    links.extend(page_links)
                    
                    if not page_links and page_number > 0:
                         # –ï—Å–ª–∏ –Ω–∞ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ (–∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–π) –Ω–µ—Ç —Å—Å—ã–ª–æ–∫, —ç—Ç–æ, –≤–µ—Ä–æ—è—Ç–Ω–æ, –∫–æ–Ω–µ—Ü
                         logging.info(f"‚ÑπÔ∏è –ù–µ—Ç —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number + 1}, –∑–∞–≤–µ—Ä—à–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è '{keyword}'.")
                         break


                except Exception as e:
                     logging.warning(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number + 1} –¥–ª—è {keyword}: {e}")
                     continue # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∏ –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é

        finally:
            await browser.close()

    logging.info(f"‚úÖ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –¥–ª—è '{keyword}': {len(links)}")
    return links