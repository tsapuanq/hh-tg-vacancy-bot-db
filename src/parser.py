# parser.py
from playwright.async_api import (
    async_playwright,
    TimeoutError as PlaywrightTimeoutError,
)
from src.config import BASE_URL, REGION_ID, SCRAPER_TIMEOUT_SELECTOR
import logging


async def get_vacancy_links(keyword: str, max_pages: int = 10) -> list[str]:
    """
    –°–æ–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–∏—Å–∫–∞.
    """
    links = []
    logging.info(
        f"üîó –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É: '{keyword}' –¥–æ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü."
    )

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        try:
            page = await browser.new_page()

            for page_number in range(max_pages):
                url = f"{BASE_URL}?text={keyword}&area={REGION_ID}&page={page_number}&order_by=publication_time&search_period=1"
                logging.info(
                    f"–ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_number + 1}/{max_pages}: {url}"
                )  

                try:
                    await page.goto(
                        url, timeout=20000
                    )  
                    await page.wait_for_selector(
                        'a[data-qa="serp-item__title"]',
                        timeout=SCRAPER_TIMEOUT_SELECTOR,
                    )  
                except PlaywrightTimeoutError:
                    logging.warning(
                        f"‚è≥ –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number + 1} –¥–ª—è {keyword}. –í–æ–∑–º–æ–∂–Ω–æ, –Ω–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–π –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–Ω–∏–º–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏."
                    )
                    break  
                except Exception as e:
                    logging.warning(
                        f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number + 1} –¥–ª—è {keyword}: {e}"
                    )
                    continue 

                try:
                    items = await page.query_selector_all(
                        'a[data-qa="serp-item__title"]'
                    )
                    page_links = [
                        await item.get_attribute("href")
                        for item in items
                        if await item.get_attribute("href")
                    ]
                    logging.info(
                        f"‚ö° –ù–∞–π–¥–µ–Ω–æ {len(page_links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number + 1}"
                    )
                    links.extend(page_links)

                    if not page_links and page_number > 0:
                        logging.info(
                            f"‚ÑπÔ∏è –ù–µ—Ç —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number + 1}, –∑–∞–≤–µ—Ä—à–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è '{keyword}'."
                        )
                        break

                except Exception as e:
                    logging.warning(
                        f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number + 1} –¥–ª—è {keyword}: {e}"
                    )
                    continue  

        finally:
            await browser.close()

    logging.info(f"‚úÖ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –¥–ª—è '{keyword}': {len(links)}")
    return links
