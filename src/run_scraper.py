# #run_scraper.py
# import asyncio
# import logging
# import random
# from playwright.async_api import async_playwright
# from src.config import SEARCH_KEYWORDS
# from src.parser import get_vacancy_links
# from src.scraper import get_vacancy_details
# from src.utils import setup_logger, canonical_link
# from database import Database
# import os

# MAX_CONCURRENT_TASKS = 10

# async def scrape_single(link, semaphore, context, results, idx, total):
#     async with semaphore:
#         try:
#             page = await context.new_page()
#             logging.info(f"[{idx}/{total}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {link}")
#             data = await get_vacancy_details(link, page)
#             await page.close()
#             if data:
#                 results.append(data)

#             delay = random.uniform(1, 2)
#             logging.info(f"‚è±Ô∏è –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—Ä–æ—Å–æ–º: {delay:.2f} —Å–µ–∫.")
#             await asyncio.sleep(delay)

#         except Exception as e:
#             logging.warning(f"[Scrape Error] {link}: {e}")

# async def run_scraper(db, mode: str = "daily"):
#     setup_logger()

#     # –ü–æ–ª—É—á–∞–µ–º —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å—Å—ã–ª–∫–∏ –∏–∑ –ë–î
#     conn = db.get_connection()
#     cursor = conn.cursor()
#     cursor.execute("SELECT url FROM vacancies")
#     existing_links = {canonical_link(row[0]) for row in cursor.fetchall()}
#     db.return_connection(conn)

#     logging.info(f"üîç –†–µ–∂–∏–º –∑–∞–ø—É—Å–∫–∞: {mode.upper()}")
#     logging.info("üîé –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Å—ã–ª–æ–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º...")

#     all_links = set()
#     for keyword in SEARCH_KEYWORDS:
#         max_pages = 100 if mode == "full" else 1
#         raw_links = await get_vacancy_links(keyword, max_pages=max_pages)
#         for raw in raw_links:
#             all_links.add(canonical_link(raw))

#     new_links = list(all_links - existing_links)
#     logging.info(f"üîó –ù–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(new_links)}")

#     results = []
#     semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)

#     async with async_playwright() as p:
#         browser = await p.chromium.launch(headless=True)
#         context = await browser.new_context(
#             user_agent=(
#                 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
#                 "AppleWebKit/537.36 (KHTML, like Gecko) "
#                 "Chrome/123.0.0.0 Safari/537.36"
#             ),
#             locale="ru-RU"
#         )

#         tasks = [
#             scrape_single(link, semaphore, context, results, idx, len(new_links))
#             for idx, link in enumerate(new_links, 1)
#         ]

#         await asyncio.gather(*tasks)
#         await browser.close()

#     results = [r for r in results if r is not None]

#     if results:
#         conn = db.get_connection()
#         cursor = conn.cursor()
#         for data in results:
#             cursor.execute("""
#                 INSERT INTO vacancies (
#                     title, description, url, published_date, company, location, salary, experience,
#                     employment_type, schedule, working_hours, work_format, skills
#                 ) VALUES (
#                     %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
#                 ) ON CONFLICT (url) DO NOTHING
#             """, (
#                 data['title'], data['description'], data['link'], data['published_date'],
#                 data['company'], data['location'], data['salary'], data['experience'],
#                 data['employment_type'], data['schedule'], data['working_hours'],
#                 data['work_format'], data['skills']
#             ))
#         conn.commit()
#         db.return_connection(conn)
#         logging.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(results)} –Ω–æ–≤—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π –≤ –ë–î")
#     else:
#         logging.info("‚ùå –ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")

# if __name__ == "__main__":
#     db = Database(os.getenv("DATABASE_URL"))
#     asyncio.run(run_scraper(db))



# run_scraper.py
import asyncio
import logging
import random
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω—É–∂–Ω—ã–µ —á–∞—Å—Ç–∏ –∏–∑ config
from src.config import SEARCH_KEYWORDS, MAX_CONCURRENT_TASKS
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º get_vacancy_details –∏–∑ scraper (–æ–Ω —Ç–µ–ø–µ—Ä—å –≤–∫–ª—é—á–∞–µ—Ç —á–∏—Å—Ç–∫—É)
from src.scraper import get_vacancy_details
from src.parser import get_vacancy_links
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —É—Ç–∏–ª–∏—Ç—ã
from src.utils import setup_logger, canonical_link
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–ª–∞—Å—Å Database
from database import Database
import os
from datetime import datetime # –ù—É–∂–µ–Ω –¥–ª—è NOW() –∏–ª–∏ CURRENT_TIMESTAMP, –Ω–æ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SQL —Ñ—É–Ω–∫—Ü–∏—é


# MAX_CONCURRENT_TASKS –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω –≤ config.py

async def scrape_single(link, semaphore, context, results, idx, total):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É —Å—Å—ã–ª–∫—É –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ.
    """
    async with semaphore: # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ–º—ã—Ö –∑–∞–¥–∞—á
        page = None # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º None –Ω–∞ —Å–ª—É—á–∞–π –æ—à–∏–±–∫–∏ –¥–æ page = await context.new_page()
        try:
            page = await context.new_page()
            logging.info(f"[{idx}/{total}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {link}")
            data = await get_vacancy_details(link, page) # get_vacancy_details —Ç–µ–ø–µ—Ä—å –ø–∞—Ä—Å–∏—Ç –∏ —á–∏—Å—Ç–∏—Ç
            if data:
                results.append(data) # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –µ—Å–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥ —É—Å–ø–µ—à–µ–Ω

            # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏
            delay = random.uniform(1, 3) # –£–≤–µ–ª–∏—á–∏–ª –≤–µ—Ä—Ö–Ω—é—é –≥—Ä–∞–Ω–∏—Ü—É –∑–∞–¥–µ—Ä–∂–∫–∏
            logging.info(f"‚è±Ô∏è –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—Ä–æ—Å–æ–º: {delay:.2f} —Å–µ–∫.")
            await asyncio.sleep(delay)

        except Exception as e:
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Å—ã–ª–∫–∏
            logging.warning(f"[Scrape Single Error] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {link}: {e}")
        finally:
            if page:
                await page.close() # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã Playwright


async def run_scraper(db: Database, mode: str = "daily"):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞.
    """
    setup_logger() # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è, –º–æ–∂–Ω–æ –≤—ã–∑–≤–∞—Ç—å –æ–¥–∏–Ω —Ä–∞–∑ –≤ run_all.py

    logging.info(f"üîç –†–µ–∂–∏–º –∑–∞–ø—É—Å–∫–∞ —Å–∫—Ä–µ–π–ø–µ—Ä–∞: {mode.upper()}")
    logging.info("üîé –ó–∞–≥—Ä—É–∑–∫–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Å—ã–ª–æ–∫ –∏–∑ –ë–î...")

    conn = None # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º None
    cursor = None
    existing_links = set()
    try:
        conn = db.get_connection()
        cursor = conn.cursor()
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ URL, —Ç.–∫. —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∏—Ö
        cursor.execute("SELECT url FROM vacancies")
        existing_links = {canonical_link(row[0]) for row in cursor.fetchall()}
        logging.info(f"üìä –í –ë–î –Ω–∞–π–¥–µ–Ω–æ {len(existing_links)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Å—ã–ª–æ–∫.")
    except Exception as e:
         logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Å—ã–ª–æ–∫ –∏–∑ –ë–î: {e}")
         # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Å—ã–ª–æ–∫, existing_links –æ—Å—Ç–∞–Ω–µ—Ç—Å—è –ø—É—Å—Ç—ã–º, —á—Ç–æ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º—É —Å–∫—Ä–µ–π–ø–∏–Ω–≥—É –≤—Å–µ–≥–æ.
         # –í–æ–∑–º–æ–∂–Ω–æ, —Å—Ç–æ–∏—Ç –ø—Ä–µ—Ä–≤–∞—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É.
         # –ü–æ–∫–∞ –æ—Å—Ç–∞–≤–∏–º —Ç–∞–∫, —á—Ç–æ–±—ã –ø–æ–ø—ã—Ç–∞—Ç—å—Å—è –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å.
    finally:
        if cursor:
             cursor.close()
        if conn:
            db.return_connection(conn) # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è


    logging.info("üîé –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Å—ã–ª–æ–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å HH...")

    all_links = set()
    for keyword in SEARCH_KEYWORDS:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
        max_pages = 100 if mode == "full" else 1
        # get_vacancy_links —Ç–µ–ø–µ—Ä—å –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤–∞ –∫ –æ—à–∏–±–∫–∞–º
        raw_links = await get_vacancy_links(keyword, max_pages=max_pages)
        for raw in raw_links:
            canonical = canonical_link(raw)
            if canonical: # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞
                all_links.add(canonical)

    # –ù–∞—Ö–æ–¥–∏–º –Ω–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏, –∫–æ—Ç–æ—Ä—ã—Ö –µ—â–µ –Ω–µ—Ç –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
    new_links = list(all_links - existing_links)
    logging.info(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(all_links)} —Å—Å—ã–ª–æ–∫ –≤—Å–µ–≥–æ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.")
    logging.info(f"üÜï –ù–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(new_links)}")

    # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫, –∑–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É —Å–∫—Ä–µ–π–ø–µ—Ä–∞
    if not new_links:
        logging.info("‚úÖ –ù–µ—Ç –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞.")
        return

    results = [] # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–µ—Ç–∞–ª–µ–π
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS) # –°–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á

    logging.info(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∫—Ä–µ–π–ø–∏–Ω–≥ –¥–µ—Ç–∞–ª–µ–π –¥–ª—è {len(new_links)} –Ω–æ–≤—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π (–¥–æ {MAX_CONCURRENT_TASKS} –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)...")

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True) # –ó–∞–ø—É—Å–∫–∞–µ–º –±—Ä–∞—É–∑–µ—Ä –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
        try:
            context = await browser.new_context(
                user_agent=(
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/123.0.0.0 Safari/537.36" # –ê–∫—Ç—É–∞–ª—å–Ω—ã–π User-Agent
                ),
                locale="ru-RU" # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ª–æ–∫–∞–ª–∏
            )

            # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ–π —Å—Å—ã–ª–∫–∏
            tasks = [
                scrape_single(link, semaphore, context, results, idx, len(new_links))
                for idx, link in enumerate(new_links, 1)
            ]
            await asyncio.gather(*tasks) # –û–∂–∏–¥–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á

        finally:
             await browser.close() # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ –±—Ä–∞—É–∑–µ—Ä–∞ Playwright

    # –§–∏–ª—å—Ç—Ä—É–µ–º None —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–≤–∞–∫–∞–Ω—Å–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å)
    successful_results = [r for r in results if r is not None]
    logging.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–µ—Ç–∞–ª–µ–π –¥–ª—è {len(successful_results)} –≤–∞–∫–∞–Ω—Å–∏–π.")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    if successful_results:
        conn = None
        cursor = None
        try:
            conn = db.get_connection()
            cursor = conn.cursor()
            # –ù–∞—á–∞–ª–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≤—Å—Ç–∞–≤–∫–∏
            # conn.autocommit = False # –ú–æ–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å autocommit –≤ False –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å conn.commit()/rollback()

            for data in successful_results:
                # –í—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ. –ò—Å–ø–æ–ª—å–∑—É–µ–º ON CONFLICT (url) DO NOTHING,
                # —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫, –µ—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –≤–¥—Ä—É–≥ —É–∂–µ –æ–∫–∞–∑–∞–ª–∞—Å—å –≤ –ë–î (—Ö–æ—Ç—è –º—ã —Ñ–∏–ª—å—Ç—Ä—É–µ–º –≤—ã—à–µ).
                # –î–æ–±–∞–≤–ª–µ–Ω—ã –ø–æ–ª—è is_relevant, processed_at, sent_to_telegram —Å –Ω–∞—á–∞–ª—å–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.
                cursor.execute("""
                    INSERT INTO vacancies (
                        title, description, url, published_date, company, location, salary, salary_range, experience,
                        employment_type, schedule, working_hours, work_format, skills,
                        is_relevant, processed_at, sent_to_telegram, published_at -- –î–æ–±–∞–≤–ª–µ–Ω—ã –ø–æ–ª—è
                    ) VALUES (
                        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                        NULL, NOW(), FALSE, %s -- NULL –¥–ª—è is_relevant (–±—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω LLM), NOW() –¥–ª—è processed_at, FALSE –¥–ª—è sent_to_telegram. published_at –∏–∑ —Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                    ) ON CONFLICT (url) DO NOTHING; -- –¢–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π –≤ –∫–æ–Ω—Ü–µ
                """, (
                    data['title'], data['description'], data['url'], data['published_date'],
                    data['company'], data['location'], data['salary'], data['salary_range'], data['experience'],
                    data['employment_type'], data['schedule'], data['working_hours'],
                    data['work_format'], data['skills'],
                    data['published_at'] # –ó–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è published_at (date object or None)
                ))

            conn.commit() # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ–º –≤—Å–µ –≤—Å—Ç–∞–≤–∫–∏
            logging.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(successful_results)} –Ω–æ–≤—ã—Ö/–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π –≤ –ë–î.")
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î: {e}")
            if conn:
                conn.rollback() # –û—Ç–∫–∞—Ç—ã–≤–∞–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        finally:
            if cursor:
                 cursor.close()
            if conn:
                db.return_connection(conn) # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
    else:
        logging.info("‚ùå –ù–µ—Ç —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.")

# if __name__ == "__main__": —É–¥–∞–ª–µ–Ω