# run_scraper.py
import asyncio
import logging
import random
from playwright.async_api import (
    async_playwright,
    TimeoutError as PlaywrightTimeoutError,
)
from src.config import SEARCH_KEYWORDS, MAX_CONCURRENT_TASKS
from src.scraper import get_vacancy_details
from src.parser import get_vacancy_links
from src.utils import setup_logger, canonical_link
from database import Database
import os
from datetime import (
    datetime,
)  

async def scrape_single(link, semaphore, context, results, idx, total):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É —Å—Å—ã–ª–∫—É –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ.
    """
    async with semaphore:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ–º—ã—Ö –∑–∞–¥–∞—á
        page = None  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º None –Ω–∞ —Å–ª—É—á–∞–π –æ—à–∏–±–∫–∏ –¥–æ page = await context.new_page()
        try:
            page = await context.new_page()
            logging.info(f"[{idx}/{total}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {link}")
            data = await get_vacancy_details(
                link, page
            )  
            if data:
                results.append(data)  
            delay = random.uniform(1, 3)  
            logging.info(f"‚è±Ô∏è –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—Ä–æ—Å–æ–º: {delay:.2f} —Å–µ–∫.")
            await asyncio.sleep(delay)

        except Exception as e:
            logging.warning(f"[Scrape Single Error] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {link}: {e}")
        finally:
            if page:
                await page.close()  


async def run_scraper(db: Database, mode: str = "daily"):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞.
    """
    setup_logger()  

    logging.info(f"üîç –†–µ–∂–∏–º –∑–∞–ø—É—Å–∫–∞ —Å–∫—Ä–µ–π–ø–µ—Ä–∞: {mode.upper()}")
    logging.info("üîé –ó–∞–≥—Ä—É–∑–∫–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Å—ã–ª–æ–∫ –∏–∑ –ë–î...")

    conn = None  
    cursor = None
    existing_links = set()
    try:
        conn = db.get_connection()
        cursor = conn.cursor()
        
        cursor.execute("SELECT url FROM vacancies")
        existing_links = {canonical_link(row[0]) for row in cursor.fetchall()}
        logging.info(f"üìä –í –ë–î –Ω–∞–π–¥–µ–Ω–æ {len(existing_links)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Å—ã–ª–æ–∫.")
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Å—ã–ª–æ–∫ –∏–∑ –ë–î: {e}")
    finally:
        if cursor:
            cursor.close()
        if conn:
            db.return_connection(conn)  

    logging.info("üîé –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Å—ã–ª–æ–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å HH...")

    all_links = set()
    for keyword in SEARCH_KEYWORDS:
        max_pages = 100 if mode == "full" else 1
        raw_links = await get_vacancy_links(keyword, max_pages=max_pages)
        for raw in raw_links:
            canonical = canonical_link(raw)
            if canonical:  
                all_links.add(canonical)

    new_links = list(all_links - existing_links)
    logging.info(f"üîó –ù–∞–π–¥–µ–Ω–æ {len(all_links)} —Å—Å—ã–ª–æ–∫ –≤—Å–µ–≥–æ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.")
    logging.info(f"üÜï –ù–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(new_links)}")

    if not new_links:
        logging.info("‚úÖ –ù–µ—Ç –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞.")
        return

    results = []  
    semaphore = asyncio.Semaphore(
        MAX_CONCURRENT_TASKS
    )  

    logging.info(
        f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∫—Ä–µ–π–ø–∏–Ω–≥ –¥–µ—Ç–∞–ª–µ–π –¥–ª—è {len(new_links)} –Ω–æ–≤—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π (–¥–æ {MAX_CONCURRENT_TASKS} –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)..."
    )

    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True
        )  
        try:
            context = await browser.new_context(
                user_agent=(
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/123.0.0.0 Safari/537.36"  
                ),
                locale="ru-RU",  
            )
            tasks = [
                scrape_single(link, semaphore, context, results, idx, len(new_links))
                for idx, link in enumerate(new_links, 1)
            ]
            await asyncio.gather(*tasks)  

        finally:
            await browser.close()  

    successful_results = [r for r in results if r is not None]
    logging.info(
        f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–µ—Ç–∞–ª–µ–π –¥–ª—è {len(successful_results)} –≤–∞–∫–∞–Ω—Å–∏–π."
    )

    if successful_results:
        conn = None
        cursor = None
        try:
            conn = db.get_connection()
            cursor = conn.cursor()

            for data in successful_results:
                cursor.execute(
                    """
                    INSERT INTO vacancies (
                        title, description, url, published_date, company, location, salary, salary_range, experience,
                        employment_type, schedule, working_hours, work_format, skills,
                        is_relevant, processed_at, sent_to_telegram, published_at -- –î–æ–±–∞–≤–ª–µ–Ω—ã –ø–æ–ª—è
                    ) VALUES (
                        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                        NULL, NOW(), FALSE, %s -- NULL –¥–ª—è is_relevant (–±—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω LLM), NOW() –¥–ª—è processed_at, FALSE –¥–ª—è sent_to_telegram. published_at –∏–∑ —Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                    ) ON CONFLICT (url) DO NOTHING; -- –¢–æ—á–∫–∞ —Å –∑–∞–ø—è—Ç–æ–π –≤ –∫–æ–Ω—Ü–µ
                """,
                    (
                        data["title"],
                        data["description"],
                        data["url"],
                        data["published_date"],
                        data["company"],
                        data["location"],
                        data["salary"],
                        data["salary_range"],
                        data["experience"],
                        data["employment_type"],
                        data["schedule"],
                        data["working_hours"],
                        data["work_format"],
                        data["skills"],
                        data[
                            "published_at"
                        ],  
                    ),
                )

            conn.commit()  
            logging.info(
                f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(successful_results)} –Ω–æ–≤—ã—Ö/–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π –≤ –ë–î."
            )
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î: {e}")
            if conn:
                conn.rollback()  
        finally:
            if cursor:
                cursor.close()
            if conn:
                db.return_connection(conn)  
    else:
        logging.info("‚ùå –ù–µ—Ç —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.")
